---
title: "Informe Hoja de trabajo 2"
author: "Marco Ramirez 195888, Alfredo Quezada, Estuardo Hernandez"
date: "16/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```


# Hoja de Trabajo: 2, Clustering

### 1. Haga el preprocesamiento del dataset, explique qué variables no aportan información a la generación de grupos y por qué. Describa con qué variables calculará los grupos
```{r wid}

library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(ggrepel)

#Preprocesamiento 
datos<-read.csv("movies.csv")

#Normalizar datos

datos<-datos[complete.cases(read.csv("movies.csv")),]
popularidad<-datos[,'popularity']
presupuesto<-datos[,'budget']
ingresos<-datos[,'revenue']
duracion<-datos[,'runtime']
conteoVotos<-datos[,'voteCount']
cle<-data.frame(popularidad,presupuesto,ingresos,duracion,conteoVotos)
clusteringVar<-scale(cle)

```
Las variables que no se usaron para el agrupamiento fueron las variables categóricas porque no son las adecuadas para el análisis. Sin embargo se puede analizar pero usando o tomando como base las numéricas. Para establcer relaciones.<br/>

Se usaron las siguientes variables:<br/> popularity: Que es el índice de popularidad de la película calculado semanalmente.<br/> budget: El presupuesto para la película.<br/> revenue: El ingreso de la película.<br/> runtime: La duración de la película.<br/> voteCount: El número de votos en la plataforma para la película.<br/> 

### 2. Analice la tendencia al agrupamiento usando el estadístico de Hopkings y la VAT (Visual Assessment of cluster Tendency) Discuta sus resultados e impresiones. 

```{r} 

hopkins(clusteringVar)

#Matriz de distancia
datos_dist<- dist(clusteringVar)

```
Como se puede ver, el valor del estadistico de Hopkins tiene un valor muy lejano a 0.5, por lo que no son datos aleatorios, lo que nos podria facilitar un agrupamiento.  

Ahora, usando un metodo grafico, si nos basamos en la grafica siguiente: 

```{r}
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
fviz_dist(datos_dist, show_labels = F)

```

Como se puede observar en la VAT, existen patrones, algunos definidos, por lo que se ratifica que el dato que arroja el estadistico de Hopkins, es correcto.




### 3. Determine cuál es el número de grupos a formar más adecuado para los datos que está trabajando. 
Haga una gráfica de codo y explique la razón de la elección de la cantidad de clústeres con la que 
trabajará.


``` {r}
#Metodo de codo
wss=0
for (i in 1:10) 
  wss[i] <- sum(kmeans(cle, centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")


```
Basandonos en el metodo de codo, el numero perfecto para trabajar es de 3, ya que, siguiendo la teoria del metodo, el punto de inflexion es igual a 3.  



```{r}

#Metodo de silueta
fviz_nbclust(clusteringVar, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")

```
Al igual que el metodo del codo, el numero de clusters ideal es de 3, pero para este metodo, se tomo en cuenta el numero de agrupaciones que tenga una silueta mas elevada. 

### 4. Utilice 3 algoritmos existentes para agrupamiento. Compare los resultados generados por cada uno. 
#### K-means

K-means es un algoritmo de clasificación no supervisada (clusterización) que agrupa objetos en k grupos basándose en sus características. El agrupamiento se realiza minimizando la suma de distancias entre cada objeto y el centroide de su grupo o cluster. Se suele usar la distancia cuadrática.
 
```{r}


km<-kmeans(clusteringVar,3,iter.max =100)
datos$grupo<-km$cluster

plotcluster(clusteringVar,km$cluster) 
```

Como se observa en la imagen, el primer paso es escoger el numero de grupos K, en este caso fue 3 tal como se justifico anteriormente, posterior a ello se establecen k centroides en el espacio de datos.

```{r}

fviz_cluster(km, data = clusteringVar,geom = "point", ellipse.type = "norm")





```


Luego se asignan los centroides y se reubican
```{r}

silkm<-silhouette(km$cluster,dist(clusteringVar))
mean(silkm[,3]) 
Kmean<-mean(silkm[,3]) 

```
Se obtiene que la silueta de K-means es de `r mean(silkm[,3])` esto indica que tenemos un buen resultado, ya que es muy cercano a 1, siendo un resultado deseable. </br>
Y el grafico de la silueta de K-means seria la siguiente
```{r}

plot(silkm, cex.names=.4, col=1:3, border=NA)

```

#### Cluster jerarquico
El algortimo de clúster jerárquico agrupa los datos basándose en la distancia entre cada uno y buscando que los datos que están dentro de un clúster sean los más similares entre sí.

```{r}
matriz_dist<- dist(clusteringVar)
hc<-hclust(datos_dist, method = "ward.D2") #Genera el clustering jerarquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
rect.hclust(hc,k=3)

groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
datos$gruposHC<-groups



```


```{r}
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) 
Jerarquico<-mean(silhc[,3]) 


```

Como se observa la silueta del algoritmo cluster jerarquico fue de `r mean(silhc[,3])`, indicando que su valor fue muy cercano a 1, siendo un resultado deseable.</br>

Graficamente la silueta de este algoritmo seria de la siguiente manera.
```{r}
plot(silhc, cex.names=.4, col=1:3, border = NA)
```


#### Mezcla de gaussiano
Este algoritmo establece que todos los puntos de datos generados se derivan de una mezcla de distribuciones gaussianas finitas que no tiene parámetros conocidos.

```{r}
mc<-Mclust(clusteringVar,3)



datos$mxGau<-mc$classification
silmg<-silhouette(mc$classification,datos_dist)
mean(silmg[,3]) 
Gaussiano<-mean(silmg[,3]) 

```
Como se observa con este algoritmo obtenimos `r mean(silmg[,3])` de promedio de la silueta, sin embargo, en este caso el valor es mayor a 0 pero no muy cercano a 1, esto debido a que posea siluetas negativas tal como se observa en el siguiente grafico. 
</br>
Graficamente se veria de la siguiente manera.

```{r}
plot(silmg, cex.names=.4, col=1:3, border = NA)
```

#### 5.Determine la calidad del agrupamiento hecho por cada algoritmo con el método de la silueta. Discuta los resultados. 

```{r}
df <- data.frame(algoritmo=c("K-mean", "Jerarquico", "Gaussiano"),
                silueta=c(Kmean, Jerarquico, Gaussiano))
head(df)
library(ggplot2)

p<-ggplot(data=df, aes(x=algoritmo, y=silueta, fill=algoritmo)) +
  geom_bar(stat="identity")
p
```

Como se observa en la grafica la que obtuvo mejores resultados fue Cluster jerarquico, con un resultado de `r Jerarquico`, podriamos decir que el Agrupamiento Jerarquico fue el mejor, ya que este cuenta con la ventaja de que cualquier medida de distancia puede ser usada, dicho sea de paso, nosotros utilizamos la matriz de distancia, por eso es que hace que se vuelva tan eficiente en comparacion a los otros metodos. 
